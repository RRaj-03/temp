agent_note = {
    'type_of_task': 'classify the task into ["summary","qna","text_generation","NLI"]',
    'enhanced_prompt' : 'the enhanced prompt ',
    'keywords' : 'keywords with most attention',
    'metric': 'metrics for the new enchanced prompt like structure, clarity, specificity, overall',
    'old_metric': 'metrics for the input prompt like structure, clarity, specificity, overall',
    }
LLM_PROMPT = f"""You are an prompt engineer specialising in enchaning the prompt. 
You have been given an intital prompt and you have to convert it to be best the possible prompt for any LLM. 
Your task is to create a Enchanced Prompt that can be used in by other LLMs and should be output in the below given format:

Agents_note format: {agent_note}

Transcript : {prompt}

Output Formatting Requirements: : 
- Output : Extract the specified details and present them in a well formatted JSON object that matches the 'Agents_note format'. 
- Use JSON syntax.
- Just output the JSON object.
"""

!pip install -q -U transformers datasets accelerate peft trl bitsandbytes xformers

from datasets import load_dataset

# Load a small portion of the Helsinki translation dataset (EN â†’ ES)
dataset = load_dataset("Helsinki-NLP/opus-mt-tc-big-en-es", split="train[:1%]")

# Convert to Mistral-style prompts
def format_example(example):
    en = example["translation"]["en"]
    es = example["translation"]["es"]
    return {
        "prompt": f"Translate the following English sentence to Spanish:\n{en}\nAnswer:",
        "response": es,
    }

dataset = dataset.map(format_example)

from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = "mistralai/Mistral-7B-v0.1"

tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token  # Set pad token

import torch
from peft import LoraConfig, get_peft_model
from transformers import BitsAndBytesConfig

# 8-bit setup using bitsandbytes
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_use_double_quant=True,
    bnb_8bit_quant_type="nf4",
    llm_int8_threshold=6.0,
)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    device_map="auto",
    torch_dtype=torch.float16,
)

# LoRA config
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "v_proj"],
)

model = get_peft_model(model, lora_config)

from trl import SFTTrainer
from transformers import TrainingArguments

# Prompt formatting
def formatting_func(example):
    return f"{example['prompt']} {example['response']}"

# Training args
training_args = TrainingArguments(
    output_dir="./mistral-lora-checkpoint",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    num_train_epochs=1,
    fp16=True,
    logging_steps=10,
    save_strategy="no",
    report_to="none",
)

# Initialize trainer
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    formatting_func=formatting_func,
    max_seq_length=512,
    args=training_args,
)

trainer.train()

bnb_config_qlora = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16,
)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config_qlora,
    device_map="auto",
    torch_dtype=torch.float16,
)

# QLoRA config (same as LoRA, but for 4-bit model)
lora_config_qlora = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "v_proj"]
)

model = get_peft_model(model, lora_config_qlora)

# Inference
prompt = "Translate the following English sentence to Spanish:\nWhere is the nearest train station?\nAnswer:"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
output = model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(output[0], skip_special_tokens=True))

trainer.save_model("mistral-lora-translator")
tokenizer.save_pretrained("mistral-lora-translator")
